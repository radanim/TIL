{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgv7eVfY6mBxjz+vTtmZVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radanim/TIL/blob/master/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9D%B4%EB%A1%A0_%EB%B0%8F_%EC%8B%A4%EC%8A%B5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzaSPCHLRUSu"
      },
      "outputs": [],
      "source": [
        "# 자연어처리: 자연어에서 의미있는 정보 분석, 추출하고 이해하는 일련의 기술 \n",
        "# 사례) 텍스트 요약, 자동응답, 대화시스템, 기계번역 \n",
        "# 데이터 전처리, 잘가공해서 모델 만들기, 텍스트 전처리 성능 확보 키포인트\n",
        "# 사람의 말을 노이즈없이 잘 처리하는 문제 \n",
        "# 토큰: 자연어 쪼개기, 토큰이라는 작은 단위, 의미를 가지는 단위로 정의, 보통은 기본적으로 단어\n",
        "# corpus 말뭉치, 구두점을 제외하는 토큰화 작업! 띄워쓰기 단위로 잘라내기 구두점을 빈공간으로 대체한 후에 빈칸을 기준으로 단어 토큰화 수행 \n",
        "# 구두점 제거하면 토큰이 의미를 잃어버리는 경우 발생 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk # 자연어처리 패키지 # 행렬처리 판다스로 하듯이, 리눅스상에서 프로그램 설치 명령어 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3SmR9t8RtPQ",
        "outputId": "f7747744-9a9a-4e5e-8427-7c6a64bc957c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # 기본적으로 영어처리 함수\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ30QnzERtUr",
        "outputId": "af2cfd4f-baee-493f-c70a-0ef7390bda96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy # 한국어 처리할수 있는 nlp 패키지 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wng6_ECwRtYc",
        "outputId": "e11db6bf-a3f4-4fc6-f66f-b54c835cee10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer # 파이썬 라이브러리를 활용한 토큰화 \n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "rkW2Y9EWRtb1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', word_tokenize(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuzTcOUjRtgJ",
        "outputId": "99170471-57ed-4442-ba50-0f28950ac0b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['Please', 'do', \"n't\", 'make', 'a', 'disturbance', 'in', 'the', 'classroom', ',', 'because', 'today', 'is', 'Teacher', \"'s\", 'Day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " The two most important days in your life are the day you are born and the day you find out why."
      ],
      "metadata": {
        "id": "uvSET6vMRti0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', word_tokenize(\" The two most important days in your life are the day you are born and the day you find out why.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCgJB7XzRtmN",
        "outputId": "a60ad52f-f13a-4fb4-9456-5f8033d3f302"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['The', 'two', 'most', 'important', 'days', 'in', 'your', 'life', 'are', 'the', 'day', 'you', 'are', 'born', 'and', 'the', 'day', 'you', 'find', 'out', 'why', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', WordPunctTokenizer().tokenize(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\")) # 구두점 기준으로 잘라내기 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssbjCcx1Rtpm",
        "outputId": "c0f71a67-4979-4d16-dec8-45ac468026cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['Please', 'don', \"'\", 't', 'make', 'a', 'disturbance', 'in', 'the', 'classroom', ',', 'because', 'today', 'is', 'Teacher', \"'\", 's', 'Day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', text_to_word_sequence(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\")) #구두점 다 지워버림 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTi31qS6RttT",
        "outputId": "f2ce45e5-f02d-47e3-f55a-5672c16f092a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['please', \"don't\", 'make', 'a', 'disturbance', 'in', 'the', 'classroom', 'because', 'today', 'is', \"teacher's\", 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Penn Treebank Tokenization 보편적인 알고리즘\n",
        "#1) 하이픈 구성 단어는 하나로 유지 \n",
        "#2) doesn't와 같이 어포스트로피로 접어가 함께하는 단어는 분리 "
      ],
      "metadata": {
        "id": "pXWMCt3fW75o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Plant-based doesn't always mean healthy.\"\n",
        "print('Treebank Tokenizer:',tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj06CBgmW78l",
        "outputId": "e3b424ae-67ca-43b5-d900-a47e334907b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenizer: ['Plant-based', 'does', \"n't\", 'always', 'mean', 'healthy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하는 corpus, language, special character에 따라 규칙이 달라짐.\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "5K7EpDyfW7_U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"The punctuation in electronic health records does not always follow standard forms. Sometimes consecutive sentences in a report have a missing space after the period of the first sentence, which can cause the sentence tokenizer to treat both sentences together as a single run-on sentence. ClarityNLP detects these occurrences and separates the sentences. It also avoids separating valid abbreviations such as C.Diff., G.Jones, etc.\"\n",
        "print('sentence tokenization:',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1KRmIvFW8CU",
        "outputId": "92f42bf0-580a-410e-d780-ef48c272264c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence tokenization: ['The punctuation in electronic health records does not always follow standard forms.', 'Sometimes consecutive sentences in a report have a missing space after the period of the first sentence, which can cause the sentence tokenizer to treat both sentences together as a single run-on sentence.', 'ClarityNLP detects these occurrences and separates the sentences.', 'It also avoids separating valid abbreviations such as C.Diff., G.Jones, etc.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTuniU_zW8Eq",
        "outputId": "568f27ce-c638-48d3-8809-b8bc7e386747"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.6.2.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.14.0)\n",
            "Building wheels for collected packages: kss\n",
            "kss\n",
            "kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.6.2-py3-none-any.whl size=42448618 sha256=d5729b7f5b0be2e6d4fff0d8b2f2e4ba15a8a97ebb9093dcdae388d93dfb1b85\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/96/36/e0934715782ac3fa2970b7a4aa4cf86725fc726bee64fdcf14\n",
            "Successfully built kss\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.2.0 kss-3.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text=\"인공지능을 활용한 자연어처리를 학습중 입니다. 그러나 한글 자연어 처리는 영어 보다 더 복잡하고 어렵습니다.\"\n",
        "print('korean tokenization:',kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC02APBMW8G9",
        "outputId": "7a93ce90-14f7-4591-bc0a-c1e5f4e936b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean tokenization: ['인공지능을 활용한 자연어처리를 학습중 입니다.', '그러나 한글 자연어 처리는 영어 보다 더 복잡하고 어렵습니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging(품사 태깅)\n",
        "# fly, 못 "
      ],
      "metadata": {
        "id": "O7tGQt_6W8JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text=\"I am actively looking for Ph.D. students, and you are a Ph.D. student.\"\n",
        "tokenized_sentence= word_tokenize(text)\n",
        "\n",
        "print('word tokenization:', tokenized_sentence)\n",
        "print('POS tagging:', pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4myiGZuW8Lh",
        "outputId": "1c83e9a5-90f6-4066-8820-1613530a92e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokenization: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', ',', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "POS tagging: [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), (',', ','), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Penn Treebank POG Tags\n",
        "# 태그마다 설명     "
      ],
      "metadata": {
        "id": "R1_QEpNoW8Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma \n",
        "\n",
        "okt=Okt()\n",
        "kkma=Kkma() #꼬꼬마 서울대에서 개발 \n",
        "# 완벽하지는 않은 한국어 자연어 처리, 사전 기준이기 때문에 생략된 부분은 잘못 분석되기도 함. 조사, 어간이 중요 \n",
        "# 자연어 처리의 어려운 문제: 동음 이의어 처리, 은유적인 표현 \n",
        "print('OKT 형태소 분석:', okt.morphs('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 품사 태깅:', okt.pos('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 명사 추출:', okt.nouns('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!')) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ru1nOBqW8Tk",
        "outputId": "7c624f10-3e2b-4fed-eb6c-eea2243c39ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석: ['인공', '지능', '공부', '는', '정말', '짜증나고', ',', '지겹고', ',', '복잡해', '!']\n",
            "OKT 품사 태깅: [('인공', 'Noun'), ('지능', 'Noun'), ('공부', 'Noun'), ('는', 'Josa'), ('정말', 'Noun'), ('짜증나고', 'Adjective'), (',', 'Punctuation'), ('지겹고', 'Adjective'), (',', 'Punctuation'), ('복잡해', 'Adjective'), ('!', 'Punctuation')]\n",
            "OKT 명사 추출: ['인공', '지능', '공부', '정말']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('OKT 형태소 분석:', kkma.morphs('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 품사 태깅:', kkma.pos('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 명사 추출:', kkma.nouns('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!')) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JEaE-kJW8V6",
        "outputId": "f606b1ca-700d-4f5e-9ed4-3e6f856d206a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석: ['인공지능', '공부', '는', '정말', '짜증나', '고', ',', '지겹', '고', ',', '복잡', '하', '어', '!']\n",
            "OKT 품사 태깅: [('인공지능', 'NNG'), ('공부', 'NNG'), ('는', 'JX'), ('정말', 'MAG'), ('짜증나', 'VV'), ('고', 'ECE'), (',', 'SP'), ('지겹', 'VA'), ('고', 'ECE'), (',', 'SP'), ('복잡', 'NNG'), ('하', 'XSV'), ('어', 'ECS'), ('!', 'SF')]\n",
            "OKT 명사 추출: ['인공', '인공지능', '지능', '공부', '복잡']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대소문자는 아스키 코드 값이 다르므로, 컴퓨터는 다르게 인식. 별개의 값으로 인식하고 카운트 \n",
        "# 대소문자 고려 중요한 문제. 성능에 영향 \n",
        "# 동음이의어 처리하면서 다양한 의미중에 결정\n",
        "# 자연어 noise, cleansing 범위는 도메인 문서에 따라 달라짐. 형태소 분석기가 달라짐.   "
      ],
      "metadata": {
        "id": "M92LndOjW8Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming(어간 추출) 줄기의 마디를 자르듯이 \n",
        "# 뿌리 단어를 찾아서 통합해주기 ex) am, are, is > be \n",
        "# 접사(affix) 단어에 추가적인 의미 부여 \n"
      ],
      "metadata": {
        "id": "fIBi_LRFW8cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyy0GA4JRtxJ",
        "outputId": "4ecd6129-6cc6-4864-8a81-f9f15558b9eb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words=['policy','doing','organization','have','going','love','lives','dies','has','starting']\n",
        "\n",
        "print('Before lemmatization:', words)\n",
        "print('After lemmatization:', [lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKRxEMCSRt0T",
        "outputId": "90e1748a-7a9c-4898-df9c-1cfe89446cea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'dies', 'has', 'starting']\n",
            "After lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'dy', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization(표제어) 추출 \n",
        "# POS 태깅을 통해서 과거형 분리 \n",
        "# lemmatization with POS information \n",
        "lemmatizer.lemmatize('dies','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rgr__6YIRt36",
        "outputId": "87a01a33-6965-4817-82e0-4ab79b4344c4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('watched','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZQNrmUysRt7I",
        "outputId": "3dcbef46-4d7d-45a3-e39b-3b4efdeece19"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'watch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('has','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zKq4HYuRRt-h",
        "outputId": "be4b3da4-60b5-4d63-8caf-ba895ad43b70"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"This was not the map we found in Billy Bones's cheat, but an accurate\"\n",
        "tokenized_sentence=word_tokenize(sentence)\n",
        "\n",
        "print(\"Before stemming:\",tokenized_sentence)\n",
        "print(\"After stemming:\",[stemmer.stem(word) for word in tokenized_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZP9rqwTRuCy",
        "outputId": "64684288-9f9b-4b10-aa4b-0f7fa0511c91"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before stemming: ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'cheat', ',', 'but', 'an', 'accurate']\n",
            "After stemming: ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'cheat', ',', 'but', 'an', 'accur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#porter stemmer 규칙기반 접근 \n"
      ],
      "metadata": {
        "id": "LtVDemj1RuGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "lancaster_stemmer=LancasterStemmer()\n",
        "\n",
        "words=['policy','doing','organization','have','going','love','lives','fly','dies','has','starting']\n",
        "\n",
        "print('Before lemmatization:', words)\n",
        "print('After lemmatization:', [porter_stemmer.stem(w) for w in words])\n",
        "print('Lancaster stemmer:',[lancaster_stemmer.stem(w) for w in words])\n",
        "\n",
        "# stemmer마다 다른 특징: 규칙기반 vs 러닝기반\n",
        "# steming 단어의 root 찾기 \n",
        "# lemmatization 표제어 추출\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7OGbuThRuJm",
        "outputId": "ce620bc4-cf43-4dca-a8e2-e3bd567c1ff2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'has', 'starting']\n",
            "After lemmatization: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'ha', 'start']\n",
            "Lancaster stemmer: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt\n",
        "# stopword(불용어) - 큰 의미가 ㅇ벗는 토큰, 유의미한 단어 선별을 위해 제거 \n",
        "# NLTK에서는 100개 이상 단어를 불용어로 패키지 내에 미리 정의함"
      ],
      "metadata": {
        "id": "_NqHSB0pRuMm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words_list=stopwords.words('english')\n",
        "print('Number of stopword:',len(stop_words_list))\n",
        "print('Print 10 stopwords:',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4AtetHRuPH",
        "outputId": "107c4a75-21a2-4e85-bfe8-79246207b5a2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stopword: 179\n",
            "Print 10 stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소문자 converting해서 지우기, 불용어 분류 전부 소문자 기준\n",
        "example=\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised\"\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "word_tokens=word_tokenize(example)\n",
        "result=[]\n",
        "for word in word_tokens:\n",
        "  if word not in stop_words:\n",
        "    result.append(word)\n",
        "\n",
        "print('Before stopword:', word_tokens)\n",
        "print('After stopword:', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVsUCk2dRuR9",
        "outputId": "2fa81a52-d557-4f70-bad1-ba15d9263240"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before stopword: ['Deep', 'learning', '(', 'also', 'known', 'as', 'deep', 'structured', 'learning', ')', 'is', 'part', 'of', 'a', 'broader', 'family', 'of', 'machine', 'learning', 'methods', 'based', 'on', 'artificial', 'neural', 'networks', 'with', 'representation', 'learning', '.', 'Learning', 'can', 'be', 'supervised', ',', 'semi-supervised', 'or', 'unsupervised']\n",
            "After stopword: ['Deep', 'learning', '(', 'also', 'known', 'deep', 'structured', 'learning', ')', 'part', 'broader', 'family', 'machine', 'learning', 'methods', 'based', 'artificial', 'neural', 'networks', 'representation', 'learning', '.', 'Learning', 'supervised', ',', 'semi-supervised', 'unsupervised']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text=\"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is\"\n",
        "tokenizer1=RegexpTokenizer(\"[\\w]+\")\n",
        "tokenizer2=RegexpTokenizer(\"[\\s]+\",gaps=True)\n",
        "\n",
        "print(tokenizer1.tokenize(text))\n",
        "print(tokenizer2.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ1TaaQzRuUz",
        "outputId": "432c783b-32cb-40eb-e3dc-cc5b85483e80"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is']\n",
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphanage', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://regex101.com/\n",
        "# 정규표현식 보는 사이트 참고 "
      ],
      "metadata": {
        "id": "F91mcZFbRuXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text encoding : text를 다른 형태로 변환(주로 숫자)\n",
        "# integer, one-hot encoding \n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sample_text=\"\"\"The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, hence the \"structured\" part.\"\"\"\n",
        "\n",
        "#token\n",
        "sentences=sent_tokenize(sample_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y-tXRFKRuaI",
        "outputId": "f9fde05d-5643-48b4-eb24-862505a7dfbd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The adjective \"deep\" in deep learning refers to the use of multiple layers in the network.', 'Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.', 'Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions.', 'In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, hence the \"structured\" part.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "preprocessed_sentences=[]\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "  # 단어 토큰화\n",
        "  tokenized_sentence=word_tokenize(sentence)\n",
        "  result=[]\n",
        "\n",
        "  for word in tokenized_sentence:\n",
        "    word=word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다. \n",
        "    if word not in stop_words: # 단어 토큰화된 결과에 대해서 불용어를 제거한다. \n",
        "      if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word]=0\n",
        "        vocab[word] +=1\n",
        "    preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjvinMFZRuc0",
        "outputId": "9a846327-1284-47ac-c867-f34dc00bf635"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('voca:',vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuxLvbZRuff",
        "outputId": "dc21dd29-fe4a-42e2-8fb6-577f14fc2015"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voca: {'adjective': 1, 'deep': 4, 'learning': 3, 'refers': 1, 'use': 1, 'multiple': 1, 'layers': 3, 'network': 2, 'early': 1, 'work': 1, 'showed': 1, 'linear': 1, 'perceptron': 1, 'universal': 1, 'classifier': 1, 'nonpolynomial': 1, 'activation': 1, 'function': 1, 'one': 1, 'hidden': 1, 'layer': 1, 'unbounded': 2, 'width': 1, 'modern': 1, 'variation': 1, 'concerned': 1, 'number': 1, 'bounded': 1, 'size': 1, 'permits': 1, 'practical': 1, 'application': 1, 'optimized': 1, 'implementation': 1, 'retaining': 1, 'theoretical': 1, 'universality': 1, 'mild': 1, 'conditions': 1, 'also': 1, 'permitted': 1, 'heterogeneous': 1, 'deviate': 1, 'widely': 1, 'biologically': 1, 'informed': 1, 'connectionist': 1, 'models': 1, 'sake': 1, 'efficiency': 1, 'trainability': 1, 'understandability': 1, 'hence': 1, 'structured': 1, 'part': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"layers\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HZ9BhMvRulJ",
        "outputId": "a32bb9ad-a50a-4f49-d82f-0ec60ed0c4c9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted=sorted(vocab.items(),key=lambda x:x[1], reverse=True) # lambda x vs x+1 비교 내림차순 정렬 \n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULyuIcVORun5",
        "outputId": "6380c58f-bcd2-4ee0-b844-389174b0c0a4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('deep', 4), ('learning', 3), ('layers', 3), ('network', 2), ('unbounded', 2), ('adjective', 1), ('refers', 1), ('use', 1), ('multiple', 1), ('early', 1), ('work', 1), ('showed', 1), ('linear', 1), ('perceptron', 1), ('universal', 1), ('classifier', 1), ('nonpolynomial', 1), ('activation', 1), ('function', 1), ('one', 1), ('hidden', 1), ('layer', 1), ('width', 1), ('modern', 1), ('variation', 1), ('concerned', 1), ('number', 1), ('bounded', 1), ('size', 1), ('permits', 1), ('practical', 1), ('application', 1), ('optimized', 1), ('implementation', 1), ('retaining', 1), ('theoretical', 1), ('universality', 1), ('mild', 1), ('conditions', 1), ('also', 1), ('permitted', 1), ('heterogeneous', 1), ('deviate', 1), ('widely', 1), ('biologically', 1), ('informed', 1), ('connectionist', 1), ('models', 1), ('sake', 1), ('efficiency', 1), ('trainability', 1), ('understandability', 1), ('hence', 1), ('structured', 1), ('part', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency >1 : # 빈도수가 작은 단어는 제외, \n",
        "    i = i+1\n",
        "  word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ4K4iFSqKl9",
        "outputId": "f5e760c4-681b-4351-f9d0-2dfb003f90c5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': 1, 'learning': 2, 'layers': 3, 'network': 4, 'unbounded': 5, 'adjective': 5, 'refers': 5, 'use': 5, 'multiple': 5, 'early': 5, 'work': 5, 'showed': 5, 'linear': 5, 'perceptron': 5, 'universal': 5, 'classifier': 5, 'nonpolynomial': 5, 'activation': 5, 'function': 5, 'one': 5, 'hidden': 5, 'layer': 5, 'width': 5, 'modern': 5, 'variation': 5, 'concerned': 5, 'number': 5, 'bounded': 5, 'size': 5, 'permits': 5, 'practical': 5, 'application': 5, 'optimized': 5, 'implementation': 5, 'retaining': 5, 'theoretical': 5, 'universality': 5, 'mild': 5, 'conditions': 5, 'also': 5, 'permitted': 5, 'heterogeneous': 5, 'deviate': 5, 'widely': 5, 'biologically': 5, 'informed': 5, 'connectionist': 5, 'models': 5, 'sake': 5, 'efficiency': 5, 'trainability': 5, 'understandability': 5, 'hence': 5, 'structured': 5, 'part': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=3\n",
        "\n",
        "#인덱스가 3 초과인 단어 제거\n",
        "words_frequency=[word for word, index in word_to_index.items() if index >= vocab_size+1]\n",
        "\n",
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "  del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZI68HlqKqV",
        "outputId": "fe03992a-72d2-4c53-bff3-75a131cfdeb7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': 1, 'learning': 2, 'layers': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV']=len(word_to_index)+1\n",
        "print(word_to_index)\n",
        "# OOV out of vocabulary 처리 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJNJHdArqKt1",
        "outputId": "9b35707c-bab7-40b9-a9bb-4ba1abec7d42"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': 1, 'learning': 2, 'layers': 3, 'OOV': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences=[]\n",
        "for sentence in preprocessed_sentences:\n",
        "  encoded_sentence=[]\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      #단어 집합에 있는 단어라면 해당 단어의 정수를 리턴,\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "  print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vS979UaqKxG",
        "outputId": "39df96bb-fe87-4ee9-9a07-6913b21b1944"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 딥러닝 모델 매트릭스 계산 정사각행렬 형태 \n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "vocab=FreqDist(np.hstack(preprocessed_sentences))\n",
        "print(vocab[\"learning\"]) #'learning'라는 단어의 빈도수 출력\n",
        "\n",
        "vocab_size=5\n",
        "vocab=vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)\n",
        "\n",
        "word_to_index={word[0]:index+1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSBuam20qK1B",
        "outputId": "e4c9f46b-b490-4f37-87f8-9d38d8c3fe54"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "[('deep', 111), ('learning', 92), ('layers', 92), ('unbounded', 67), ('network', 51)]\n",
            "{'deep': 1, 'learning': 2, 'layers': 3, 'unbounded': 4, 'network': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "preprocessed_sent=[['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part']]\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "# 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 tokenizer의 인자 oov_token을 사용\n",
        "# 숫자 0과 oov를 고려해서 집합\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
        "\n",
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+1)\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+2,oov_token='OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "print('단어 OOV의 인덱스:()'.format(tokenizer.word_index['OOV']))\n",
        "\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00gyPcq6qK4H",
        "outputId": "197ac2cd-621d-47d3-8a64-678eafd2627a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
            "단어 OOV의 인덱스:()\n",
            "[[1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt=Okt()\n",
        "tokens=okt.morphs('나는 자연어 처리를 배운다')\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BF1AP1cqK7e",
        "outputId": "a1259665-98b7-4cc7-b693-3427cb35552b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={word:index for index, word in enumerate(tokens)}\n",
        "print('단어 집합:',word_to_index)\n",
        "\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index))\n",
        "  index=word_to_index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector\n",
        "\n",
        "one_hot_encoding(\"자연어\", word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFb5T5Q2qK_O",
        "outputId": "dfa456ba-238b-4a2d-a37c-3421e7a77f00"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index))\n",
        "  index=word_to_index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector\n",
        "\n",
        "  one_hot_encoding(\"자연어\", word_to_index)"
      ],
      "metadata": {
        "id": "DfIfgZuVqLDE"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print('단어 집합:', tokenizer.word_index)\n",
        "\n",
        "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)\n",
        "\n",
        "one_hot=to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iopwf0GcqLGE",
        "outputId": "cd723fa1-759f-4364-e2cd-0b4096efaf69"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
            "[2, 5, 1, 6, 3, 7]\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#원핫 인코딩 한계\n",
        "#단어가 늘어날수록 저장하기 위한 공간이 증가\n",
        "#단어사이의 연관성을 활용하기가 어려움\n",
        "\n",
        "#padding\n",
        "#자연어 처리과정에서 각문장, 문서의 서로 길이가 다른 경우 이를 맞춰줌\n",
        "#길이가 동일해야 하나의 행렬로 묶어서 처리가능(병렬 처리)\n",
        "#여러 문장의 길이를 임의로 동일하게 맞춰주기, 병렬연산 \n"
      ],
      "metadata": {
        "id": "m7stwK5MqLJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-aehDK3anxrI",
        "outputId": "ba2275b0-0139-4335-90df-80832e76bc58"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-fi8mch9p\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-fi8mch9p\n",
            "Collecting tensorflow==2.7.2\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.2%2Bzzzcolab20220516114640-cp37-cp37m-linux_x86_64.whl (671.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 671.4 MB 1.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (3.1.0)\n",
            "Collecting argparse>=1.4.0\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.5.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (3.17.3)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.48.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (4.1.1)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.2.0)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.5-py3-none-any.whl size=2268638 sha256=2353d3d66dc00f78f098a27575f2c1ac938e2980b8488acf022b2c4ad6f923ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iiywyw4o/wheels/9b/93/81/a2a7dc8c66ede5bf30634d20635f32b95eac7ca2ea8844058b\n",
            "Successfully built pykospacing\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow, argparse, pykospacing\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed argparse-1.4.0 gast-0.4.0 keras-2.7.0 pykospacing-0.5 tensorflow-2.7.2+zzzcolab20220516114640 tensorflow-estimator-2.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "gast",
                  "keras",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent='김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 있다.'\n",
        "\n",
        "new_sent=sent.replace(\"\",'') # 띄어쓰기가 없는 문장 임의로 만들기\n",
        "print(new_sent)\n",
        "\n",
        "#PyKoSpasing 활용 문장 자동 띄어쓰기\n",
        "from pykospacing import Spacing\n",
        "spacing=Spacing()\n",
        "kospacing_sent=spacing(new_sent)\n",
        "\n",
        "print(sent)\n",
        "print(kospacing_sent)\n",
        "#step by step cleansing 필요 \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ljJlnA5qLNi",
        "outputId": "359cc667-a980-4733-b10d-a21e4118a0f4"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 있다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f934155b440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f930a4d7a50>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f934155b440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f930a4d7a50>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 있다.\n",
            "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결 전의 날을 앞두고 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lwpbb72qLRJ",
        "outputId": "ecc42392-40c2-4d27-edb9-fbdceb76b62e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-zt7tghs5\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-zt7tghs5\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2022.6.15)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4868 sha256=cd6c7a89eb8f549c8f986061ff504aaf3e2f9f83509210c4eecc3a9701ce214d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mqau5ib3/wheels/ab/f5/7b/d4124bb329c905301baed80e2ae45aa14e824f62ebc3ec2cc4\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: py-hanspell\n",
            "Successfully installed py-hanspell-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token(word, sentence)-소문자,오타,띄어쓰기 > POS-tagging-인덱싱 과정? > cleaning/normalization-noise,N/A > stmming/levanim > encoding "
      ],
      "metadata": {
        "id": "glLbJmH5qLUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "sent=\"맞춤뻡 틀리는 사람이 요즘 많치? 쓰고싶은 대로 쓰지뭐\"\n",
        "spelled_sent=spell_checker.check(sent)\n",
        "\n",
        "hanspell_sent=spelled_sent.checked\n",
        "print(hanspell_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAbBczsLqLYK",
        "outputId": "fa98dfb2-d14e-49dc-f329-645ce2b0b2eb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "맞춤법 틀리는 사람이 요즘 많지? 쓰고 싶은 대로 쓰지 뭐\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SOYNLP: POS tagging"
      ],
      "metadata": {
        "id": "rUq8Lea8qLbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soynlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mz0g248qLes",
        "outputId": "0f9187fa-bb14-4f11-b497-9ac30319c5b5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[K     |████████████████████████████████| 416 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.normalizer import *\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ',num_repeats=2))\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ',num_repeats=2))\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠ',num_repeats=1))\n",
        "\n",
        "print(repeat_normalize('와하하하하하하하하하하핫',num_repeats=2))\n",
        "print(repeat_normalize('와하하하하하하하핫',num_repeats=2))\n",
        "print(repeat_normalize('와하하하하핫',num_repeats=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbzdMf9nqLit",
        "outputId": "66e0aa36-783d-4ba3-c9b9-5e58da4a7351"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어ㅋㅋ영화존잼쓰ㅠㅠ\n",
            "어ㅋㅋ영화존잼쓰ㅠㅠ\n",
            "어ㅋㅋ영화존잼쓰ㅠㅠ\n",
            "어ㅋ영화존잼쓰ㅠ\n",
            "와하하핫\n",
            "와하하핫\n",
            "와하하핫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 전처리 패키지 \n",
        "# SOYNLP, Hanspell, kkma 등\n",
        "# 확률 기반 언어모델(SLM)\n",
        "# N-gram, TF-IDF\n",
        "# 인공신경망 기반 언어모델 - BERT,GPT3,4,KoBERT,HYPERCLOVA(네이버)"
      ],
      "metadata": {
        "id": "H4OmHlKAqLl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT를 기준으로 혁신, 룰기반 학습에서, 인공지능 관련하여 논평 작성도 가능한 수준. \n",
        "#언어모델이 두문장을 비교하여 적절 확률 계산, 비교 및 판단 - 기계번역, 오타교정,음성인식\n",
        "#통계적 언어모델 - 조건부 확률 계산 \n",
        "#n-gram: 확률을 계산하기 위한 단어의 포함범위를 조절하여 최대 확률을 가지도록 유도 \n",
        "#n: 문자의 갯수, 몇개까지 고려할지, 더 짧은 단어 시퀀스가 존재할 확률이 더 높음 \n",
        "#TF-IDF(term frequency): 단어의 빈도와 역문서 빈도를 사용하여 문서집합 내 단어들마다 중요한 정도를 가중치로 부여 \n",
        "#너무 흔한단어는 stop word 처리 \n",
        "#한문서에서 단어가 자주 등장하면 중요, 특정단어가 여러문서에서 등장하면 중요하지 않음.\n",
        "#idf와 df는 반비례 역수관계 \n"
      ],
      "metadata": {
        "id": "Y8lrIbSvqLo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # 데이터프레임 사용을 위해\n",
        "from math import log #IDF 계산을 위해\n",
        "\n",
        "docs=[\n",
        "    '먹고 싶은 사과',\n",
        "    '먹고 싶은 바나나',\n",
        "    '길고 노란 바나나 바나나',\n",
        "    '저는 과일이 좋아요'\n",
        "]\n",
        "\n",
        "vocab=list(set(w for doc in docs for w in doc.split()))\n",
        "vocab.sort()"
      ],
      "metadata": {
        "id": "qPzI6l6RqLsN"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 총 문서의 수\n",
        "N=len(docs)\n",
        "\n",
        "def tf(t,d):\n",
        "  return d.count(t)\n",
        "\n",
        "def idf(t):\n",
        "  df=0\n",
        "  for doc in docs:\n",
        "    df += t in doc\n",
        "  return log(N/(df+1))\n",
        "\n",
        "def tfidf(t,d):\n",
        "  return tf(t,d)*idf(t)"
      ],
      "metadata": {
        "id": "topZxk76qLvw"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "\n",
        "# 각 문서에 대해서 아래 연산을 반복\n",
        "for i in range(N):\n",
        "  result.append([])\n",
        "  d=docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t=vocab[j]\n",
        "    result[-1].append(tf(t,d))\n",
        "\n",
        "tf_=pd.DataFrame(result,columns=vocab)"
      ],
      "metadata": {
        "id": "8CjGtA51qLzP"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "for j in range(len(vocab)):\n",
        "  t=vocab[j]\n",
        "  result.append(idf(t))\n",
        "\n",
        "idf_=pd.DataFrame(result,index=vocab,columns=['IDF'])\n",
        "idf_\n",
        "# idf값이 클수록 중요, df값이 클수록 안중요 \n",
        "# idf값은 df값의 역수 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "KCBQQFvQRuq9",
        "outputId": "a76aaa04-06f3-4d33-d551-4c7f4076eecb"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          IDF\n",
              "과일이  0.693147\n",
              "길고   0.693147\n",
              "노란   0.693147\n",
              "먹고   0.287682\n",
              "바나나  0.287682\n",
              "사과   0.693147\n",
              "싶은   0.287682\n",
              "저는   0.693147\n",
              "좋아요  0.693147"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9482ce7-bd05-4b59-927a-f8e9e669672f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>과일이</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>길고</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>노란</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>먹고</th>\n",
              "      <td>0.287682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>바나나</th>\n",
              "      <td>0.287682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>사과</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>싶은</th>\n",
              "      <td>0.287682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>저는</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>좋아요</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9482ce7-bd05-4b59-927a-f8e9e669672f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9482ce7-bd05-4b59-927a-f8e9e669672f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9482ce7-bd05-4b59-927a-f8e9e669672f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "for i in range(N):\n",
        "  result.append([])\n",
        "  d=docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t=vocab[j]\n",
        "    result[-1].append(tfidf(t,d))\n",
        "\n",
        "tfidf_=pd.DataFrame(result,columns=vocab)\n",
        "tfidf_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "yahQKNfqwdju",
        "outputId": "441196ba-a032-48d5-be07-72c8e3476d0a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        과일이        길고        노란  ...        싶은        저는       좋아요\n",
              "0  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n",
              "2  0.000000  0.693147  0.693147  ...  0.000000  0.000000  0.000000\n",
              "3  0.693147  0.000000  0.000000  ...  0.000000  0.693147  0.693147\n",
              "\n",
              "[4 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c88184d-fb10-4131-b81c-1adeb5613886\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>과일이</th>\n",
              "      <th>길고</th>\n",
              "      <th>노란</th>\n",
              "      <th>먹고</th>\n",
              "      <th>바나나</th>\n",
              "      <th>사과</th>\n",
              "      <th>싶은</th>\n",
              "      <th>저는</th>\n",
              "      <th>좋아요</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.575364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c88184d-fb10-4131-b81c-1adeb5613886')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8c88184d-fb10-4131-b81c-1adeb5613886 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8c88184d-fb10-4131-b81c-1adeb5613886');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus=[\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do',\n",
        "]\n",
        "\n",
        "vector=CountVectorizer()\n",
        "\n",
        "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "\n",
        "# 각 단어와 맵핑된 인덱스 출력\n",
        "print(vector.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-7hJJIpwdmb",
        "outputId": "7c330f91-f55e-4ccf-fd9d-4f91fcf28128"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 0 1 0 1 1]\n",
            " [0 0 1 0 0 0 0 1 0]\n",
            " [1 0 0 0 1 0 1 0 0]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus=[\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do',\n",
        "]\n",
        "\n",
        "tfidfv=TfidfVectorizer().fit(corpus)\n",
        "print(tfidfv.transform(corpus).toarray())\n",
        "print(tfidfv.vocabulary_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK5sR0sswdpF",
        "outputId": "a633b5ca-499e-411e-c8bb-4c485e905df4"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
            "  0.         0.35543247 0.46735098]\n",
            " [0.         0.         0.79596054 0.         0.         0.\n",
            "  0.         0.60534851 0.        ]\n",
            " [0.57735027 0.         0.         0.         0.57735027 0.\n",
            "  0.57735027 0.         0.        ]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서간 유사도 측정\n",
        "# 벡터의 크기와 방향이 같은 경우: 1\n",
        "# 코사인 유사도 값 -1~1 분포 1에 가까울수록 유사함. "
      ],
      "metadata": {
        "id": "w2-r5usqwdr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도를 활용한 추천시스템 구현 실습\n",
        "# TF-IDF"
      ],
      "metadata": {
        "id": "rFUUuWLtwduC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#파일 직접 업로드하기\n",
        "from google.colab import files\n",
        "myfile=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "STZE-DUnwdwf",
        "outputId": "29b361d9-6916-4b00-f5b7-8872dd7c9b24"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff21a60d-c4ac-43d1-80c6-162523f9b3b5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff21a60d-c4ac-43d1-80c6-162523f9b3b5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving movies_metadata.csv to movies_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 국제학술대회 경진대회 KDD 여비 지원\n",
        "# 인건비 참여율 130% kisti 학생연구원 8명 알바생 고용하고 돈남아"
      ],
      "metadata": {
        "id": "tdXEOaz-wdy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "data=pd.read_csv('movies_metadata.csv',low_memory=False)\n",
        "\n",
        "#다운로드 받은 훈련 데이터에서 상위 2개의 샘플만 출력하여 데이터의 형식을 확인\n",
        "data.head(2)\n",
        "\n",
        "#좋아하는 영화를 입력하면, 해당 영화의 줄거리와 유사한 줄거리의 영화를 찾아서 추천하는 시스템 만들기 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "aewoYLPDwd1i",
        "outputId": "5aa126aa-55b8-4bad-a41c-dfef2996f05e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   adult  ... vote_count\n",
              "0  False  ...     5415.0\n",
              "1  False  ...     2413.0\n",
              "\n",
              "[2 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e5ee2a8-9ee6-4b51-9a03-983c8c851b25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adult</th>\n",
              "      <th>belongs_to_collection</th>\n",
              "      <th>budget</th>\n",
              "      <th>genres</th>\n",
              "      <th>homepage</th>\n",
              "      <th>id</th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>original_language</th>\n",
              "      <th>original_title</th>\n",
              "      <th>overview</th>\n",
              "      <th>popularity</th>\n",
              "      <th>poster_path</th>\n",
              "      <th>production_companies</th>\n",
              "      <th>production_countries</th>\n",
              "      <th>release_date</th>\n",
              "      <th>revenue</th>\n",
              "      <th>runtime</th>\n",
              "      <th>spoken_languages</th>\n",
              "      <th>status</th>\n",
              "      <th>tagline</th>\n",
              "      <th>title</th>\n",
              "      <th>video</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>vote_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
              "      <td>30000000</td>\n",
              "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
              "      <td>http://toystory.disney.com/toy-story</td>\n",
              "      <td>862</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>en</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
              "      <td>21.946943</td>\n",
              "      <td>/rhIRbceoE9lR4veEXuwCC2wARtG.jpg</td>\n",
              "      <td>[{'name': 'Pixar Animation Studios', 'id': 3}]</td>\n",
              "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
              "      <td>1995-10-30</td>\n",
              "      <td>373554033.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
              "      <td>Released</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>False</td>\n",
              "      <td>7.7</td>\n",
              "      <td>5415.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65000000</td>\n",
              "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8844</td>\n",
              "      <td>tt0113497</td>\n",
              "      <td>en</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>When siblings Judy and Peter discover an encha...</td>\n",
              "      <td>17.015539</td>\n",
              "      <td>/vzmL6fP7aPKNKPRTFnZmiUfciyV.jpg</td>\n",
              "      <td>[{'name': 'TriStar Pictures', 'id': 559}, {'na...</td>\n",
              "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
              "      <td>1995-12-15</td>\n",
              "      <td>262797249.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
              "      <td>Released</td>\n",
              "      <td>Roll the dice and unleash the excitement!</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>False</td>\n",
              "      <td>6.9</td>\n",
              "      <td>2413.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e5ee2a8-9ee6-4b51-9a03-983c8c851b25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e5ee2a8-9ee6-4b51-9a03-983c8c851b25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e5ee2a8-9ee6-4b51-9a03-983c8c851b25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문서간 유사도 측정\n",
        "#유사도를 활용한 추천시스템 구현 실습:Data preprocessing(데이터 샘플링, 결측치 처리)\n",
        "\n",
        "#상위 2만개의 샘플을 data에 저장\n",
        "data=data.head(20000)\n",
        "\n",
        "#overview 열에 존재하는 모든 결측값을 전부 카운트하여 출력\n",
        "print('overview 열의 결측값의 수:',data['overview'].isnull().sum())\n",
        "\n",
        "#결측값을 빈 값으로 대체\n",
        "data['overview']=data['overview'].fillna('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7vpCdzCwd4I",
        "outputId": "673f235a-08e1-4142-972a-1b65b4d43fa7"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overview 열의 결측값의 수: 135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 행렬 계산\n",
        "#overview 열에 대해서 TF-IDF 행렬을 구한 후 행렬의 크기를 출력\n",
        "tfidf=TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix=tfidf.fit_transform(data['overview'])\n",
        "print('TF-IDF 행렬의 크기(shape):',tfidf_matrix.shape)\n",
        "# 리뷰가 20000, 단어수가 47487"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExlcQ4juwd6i",
        "outputId": "58a814f6-60ee-45b4-c9f5-16964539fe57"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF 행렬의 크기(shape): (20000, 47487)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 행렬(벡터)를 활용하여 코사인 유사도 계산\n",
        "\n",
        "# 20,000개의 문서 벡터에 대해서 상호 간의 코사인 유사도 측정\n",
        "cosine_sim=cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
        "print('코사인 유사도 연산 결과:',cosine_sim.shape)\n",
        "\n",
        "#20,000행 20,000열의 행렬\n",
        "#즉, 20,000개의 각 문서벡터(영화줄거리)간 유사도가 측정된 결과"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO8Y4cCp-YGV",
        "outputId": "f1f80352-5dbf-4ac7-c735-c16de92afc59"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "코사인 유사도 연산 결과: (20000, 20000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영화의 타이틀을 key, 인덱스를 value로 하는 dictionary인 title_to_index 생성\n",
        "\n",
        "title_to_index=dict(zip(data['title'], data.index))\n",
        "\n",
        "#영화 제목 Father of the Bride Part 2의 인덱스를 리턴\n",
        "idx=title_to_index['Father of the Bride Part II']\n",
        "print(idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWvfJOGH-YK6",
        "outputId": "66459630-2eb4-4e71-8ae7-36e8967b2abf"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title,cosine_sim=cosine_sim):\n",
        "  #선택한 영화의 타이틀로부터 해당 영화의 인덱스를 받아온다.\n",
        "  idx=title_to_index[title]\n",
        "  # 해당 영화와 모든 영화와의 유사도를 가져온다. \n",
        "  sim_scores=list(enumerate(cosine_sim[idx]))\n",
        "  #유사도에 따라 영화들을 정렬한다.\n",
        "  sim_scores=sorted(sim_scores, key=lambda x:x[1],reverse=True)\n",
        "  # 가장 유사한 10개의 영화를 받아온다. \n",
        "  sim_scores=sim_scores[1:11]\n",
        "  # 가장 유사한 10개의 영화의 인덱스를 얻는다.\n",
        "  movie_indices=[idx[0] for idx in sim_scores]\n",
        "  #가장 유사한 10개의 영화의 제목을 리턴한다.\n",
        "  return data['title'].iloc[movie_indices]\n",
        "#영화 다크 나이트 라이즈와 overview가 유사한 영화 검색\n",
        "get_recommendations('Toy Story')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R4gt6uY-YPJ",
        "outputId": "5dbc6fce-4dc9-4fb9-9694-f2a1ed8d69ca"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15348               Toy Story 3\n",
              "2997                Toy Story 2\n",
              "10301    The 40 Year Old Virgin\n",
              "8327                  The Champ\n",
              "1071      Rebel Without a Cause\n",
              "11399    For Your Consideration\n",
              "1932                  Condorman\n",
              "3057            Man on the Moon\n",
              "485                      Malice\n",
              "11606              Factory Girl\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skip-gram > n-gram\n",
        "# one hot encoding 단어의 벡터화 \n",
        "# word2vec 만들기 \n",
        "# 형태소 분석기 mecab\n",
        "# word2vec gensim 고정 \n",
        "# 언어모델 bert gpt-3 "
      ],
      "metadata": {
        "id": "oxsX5yT2-YSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qB-Jabh_-YVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k31cSfu5-YYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YwnLVAYP-Yau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWRJIuMs-YdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzoyk2SK-Yfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mI9t8wf-YiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xy-d1IGd-Ykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiFAeFME-YnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hSedRVq-YpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htg5OtND-YrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Q7wnAbt-YuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEekAOIR-YxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BcBlfn0-Y1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1xLf0Xk-Y36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-r6mxWql-Y66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpjrzHoKwd9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvVncb9Rwd_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUSj0hz-weCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9A-BRNJweFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1OaRLvcweHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5B9gwB2JweJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yYj_Oz1weMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YuM-Uf8iwePb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qiRUQbGqweR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzT5B1LXRutp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}