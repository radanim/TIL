{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvIpyCfPW49oq1Sb2RyvRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radanim/TIL/blob/master/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9D%B4%EB%A1%A0_%EB%B0%8F_%EC%8B%A4%EC%8A%B5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzaSPCHLRUSu"
      },
      "outputs": [],
      "source": [
        "# 자연어처리: 자연어에서 의미있는 정보 분석, 추출하고 이해하는 일련의 기술 \n",
        "# 사례) 텍스트 요약, 자동응답, 대화시스템, 기계번역 \n",
        "# 데이터 전처리, 잘가공해서 모델 만들기, 텍스트 전처리 성능 확보 키포인트\n",
        "# 사람의 말을 노이즈없이 잘 처리하는 문제 \n",
        "# 토큰: 자연어 쪼개기, 토큰이라는 작은 단위, 의미를 가지는 단위로 정의, 보통은 기본적으로 단어\n",
        "# corpus 말뭉치, 구두점을 제외하는 토큰화 작업! 띄워쓰기 단위로 잘라내기 구두점을 빈공간으로 대체한 후에 빈칸을 기준으로 단어 토큰화 수행 \n",
        "# 구두점 제거하면 토큰이 의미를 잃어버리는 경우 발생 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk # 자연어처리 패키지 # 행렬처리 판다스로 하듯이, 리눅스상에서 프로그램 설치 명령어 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3SmR9t8RtPQ",
        "outputId": "f7747744-9a9a-4e5e-8427-7c6a64bc957c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # 기본적으로 영어처리 함수\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ30QnzERtUr",
        "outputId": "af2cfd4f-baee-493f-c70a-0ef7390bda96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy # 한국어 처리할수 있는 nlp 패키지 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wng6_ECwRtYc",
        "outputId": "e11db6bf-a3f4-4fc6-f66f-b54c835cee10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer # 파이썬 라이브러리를 활용한 토큰화 \n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "rkW2Y9EWRtb1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', word_tokenize(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuzTcOUjRtgJ",
        "outputId": "99170471-57ed-4442-ba50-0f28950ac0b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['Please', 'do', \"n't\", 'make', 'a', 'disturbance', 'in', 'the', 'classroom', ',', 'because', 'today', 'is', 'Teacher', \"'s\", 'Day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " The two most important days in your life are the day you are born and the day you find out why."
      ],
      "metadata": {
        "id": "uvSET6vMRti0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', word_tokenize(\" The two most important days in your life are the day you are born and the day you find out why.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCgJB7XzRtmN",
        "outputId": "a60ad52f-f13a-4fb4-9456-5f8033d3f302"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['The', 'two', 'most', 'important', 'days', 'in', 'your', 'life', 'are', 'the', 'day', 'you', 'are', 'born', 'and', 'the', 'day', 'you', 'find', 'out', 'why', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', WordPunctTokenizer().tokenize(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\")) # 구두점 기준으로 잘라내기 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssbjCcx1Rtpm",
        "outputId": "c0f71a67-4979-4d16-dec8-45ac468026cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['Please', 'don', \"'\", 't', 'make', 'a', 'disturbance', 'in', 'the', 'classroom', ',', 'because', 'today', 'is', 'Teacher', \"'\", 's', 'Day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', text_to_word_sequence(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\")) #구두점 다 지워버림 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTi31qS6RttT",
        "outputId": "f2ce45e5-f02d-47e3-f55a-5672c16f092a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['please', \"don't\", 'make', 'a', 'disturbance', 'in', 'the', 'classroom', 'because', 'today', 'is', \"teacher's\", 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Penn Treebank Tokenization 보편적인 알고리즘\n",
        "#1) 하이픈 구성 단어는 하나로 유지 \n",
        "#2) doesn't와 같이 어포스트로피로 접어가 함께하는 단어는 분리 "
      ],
      "metadata": {
        "id": "pXWMCt3fW75o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Plant-based doesn't always mean healthy.\"\n",
        "print('Treebank Tokenizer:',tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj06CBgmW78l",
        "outputId": "e3b424ae-67ca-43b5-d900-a47e334907b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenizer: ['Plant-based', 'does', \"n't\", 'always', 'mean', 'healthy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하는 corpus, language, special character에 따라 규칙이 달라짐.\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "5K7EpDyfW7_U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"The punctuation in electronic health records does not always follow standard forms. Sometimes consecutive sentences in a report have a missing space after the period of the first sentence, which can cause the sentence tokenizer to treat both sentences together as a single run-on sentence. ClarityNLP detects these occurrences and separates the sentences. It also avoids separating valid abbreviations such as C.Diff., G.Jones, etc.\"\n",
        "print('sentence tokenization:',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1KRmIvFW8CU",
        "outputId": "92f42bf0-580a-410e-d780-ef48c272264c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence tokenization: ['The punctuation in electronic health records does not always follow standard forms.', 'Sometimes consecutive sentences in a report have a missing space after the period of the first sentence, which can cause the sentence tokenizer to treat both sentences together as a single run-on sentence.', 'ClarityNLP detects these occurrences and separates the sentences.', 'It also avoids separating valid abbreviations such as C.Diff., G.Jones, etc.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTuniU_zW8Eq",
        "outputId": "568f27ce-c638-48d3-8809-b8bc7e386747"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.6.2.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.14.0)\n",
            "Building wheels for collected packages: kss\n",
            "kss\n",
            "kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.6.2-py3-none-any.whl size=42448618 sha256=d5729b7f5b0be2e6d4fff0d8b2f2e4ba15a8a97ebb9093dcdae388d93dfb1b85\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/96/36/e0934715782ac3fa2970b7a4aa4cf86725fc726bee64fdcf14\n",
            "Successfully built kss\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.2.0 kss-3.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text=\"인공지능을 활용한 자연어처리를 학습중 입니다. 그러나 한글 자연어 처리는 영어 보다 더 복잡하고 어렵습니다.\"\n",
        "print('korean tokenization:',kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC02APBMW8G9",
        "outputId": "7a93ce90-14f7-4591-bc0a-c1e5f4e936b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean tokenization: ['인공지능을 활용한 자연어처리를 학습중 입니다.', '그러나 한글 자연어 처리는 영어 보다 더 복잡하고 어렵습니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging(품사 태깅)\n",
        "# fly, 못 "
      ],
      "metadata": {
        "id": "O7tGQt_6W8JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l4myiGZuW8Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R1_QEpNoW8Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ru1nOBqW8Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JEaE-kJW8V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M92LndOjW8Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIBi_LRFW8cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zyy0GA4JRtxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKRxEMCSRt0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rgr__6YIRt36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQNrmUysRt7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKq4HYuRRt-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZP9rqwTRuCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtVDemj1RuGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7OGbuThRuJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NqHSB0pRuMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lx4AtetHRuPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVsUCk2dRuR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQ1TaaQzRuUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F91mcZFbRuXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7y-tXRFKRuaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjvinMFZRuc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOuxLvbZRuff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HZ9BhMvRulJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULyuIcVORun5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCBQQFvQRuq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzT5B1LXRutp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}