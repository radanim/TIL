{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxtGjp675A5EliqDg8reU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radanim/TIL/blob/master/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9D%B4%EB%A1%A0_%EB%B0%8F_%EC%8B%A4%EC%8A%B5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzaSPCHLRUSu"
      },
      "outputs": [],
      "source": [
        "# 자연어처리: 자연어에서 의미있는 정보 분석, 추출하고 이해하는 일련의 기술 \n",
        "# 사례) 텍스트 요약, 자동응답, 대화시스템, 기계번역 \n",
        "# 데이터 전처리, 잘가공해서 모델 만들기, 텍스트 전처리 성능 확보 키포인트\n",
        "# 사람의 말을 노이즈없이 잘 처리하는 문제 \n",
        "# 토큰: 자연어 쪼개기, 토큰이라는 작은 단위, 의미를 가지는 단위로 정의, 보통은 기본적으로 단어\n",
        "# corpus 말뭉치, 구두점을 제외하는 토큰화 작업! 띄워쓰기 단위로 잘라내기 구두점을 빈공간으로 대체한 후에 빈칸을 기준으로 단어 토큰화 수행 \n",
        "# 구두점 제거하면 토큰이 의미를 잃어버리는 경우 발생 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk # 자연어처리 패키지 # 행렬처리 판다스로 하듯이, 리눅스상에서 프로그램 설치 명령어 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3SmR9t8RtPQ",
        "outputId": "f7747744-9a9a-4e5e-8427-7c6a64bc957c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # 기본적으로 영어처리 함수\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ30QnzERtUr",
        "outputId": "af2cfd4f-baee-493f-c70a-0ef7390bda96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy # 한국어 처리할수 있는 nlp 패키지 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wng6_ECwRtYc",
        "outputId": "e11db6bf-a3f4-4fc6-f66f-b54c835cee10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer # 파이썬 라이브러리를 활용한 토큰화 \n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "rkW2Y9EWRtb1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', word_tokenize(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuzTcOUjRtgJ",
        "outputId": "99170471-57ed-4442-ba50-0f28950ac0b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['Please', 'do', \"n't\", 'make', 'a', 'disturbance', 'in', 'the', 'classroom', ',', 'because', 'today', 'is', 'Teacher', \"'s\", 'Day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " The two most important days in your life are the day you are born and the day you find out why."
      ],
      "metadata": {
        "id": "uvSET6vMRti0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', word_tokenize(\" The two most important days in your life are the day you are born and the day you find out why.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCgJB7XzRtmN",
        "outputId": "a60ad52f-f13a-4fb4-9456-5f8033d3f302"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['The', 'two', 'most', 'important', 'days', 'in', 'your', 'life', 'are', 'the', 'day', 'you', 'are', 'born', 'and', 'the', 'day', 'you', 'find', 'out', 'why', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', WordPunctTokenizer().tokenize(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\")) # 구두점 기준으로 잘라내기 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssbjCcx1Rtpm",
        "outputId": "c0f71a67-4979-4d16-dec8-45ac468026cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['Please', 'don', \"'\", 't', 'make', 'a', 'disturbance', 'in', 'the', 'classroom', ',', 'because', 'today', 'is', 'Teacher', \"'\", 's', 'Day', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Word tokenization :', text_to_word_sequence(\"Please don't make a disturbance in the classroom, because today is Teacher's Day!\")) #구두점 다 지워버림 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTi31qS6RttT",
        "outputId": "f2ce45e5-f02d-47e3-f55a-5672c16f092a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization : ['please', \"don't\", 'make', 'a', 'disturbance', 'in', 'the', 'classroom', 'because', 'today', 'is', \"teacher's\", 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Penn Treebank Tokenization 보편적인 알고리즘\n",
        "#1) 하이픈 구성 단어는 하나로 유지 \n",
        "#2) doesn't와 같이 어포스트로피로 접어가 함께하는 단어는 분리 "
      ],
      "metadata": {
        "id": "pXWMCt3fW75o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Plant-based doesn't always mean healthy.\"\n",
        "print('Treebank Tokenizer:',tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj06CBgmW78l",
        "outputId": "e3b424ae-67ca-43b5-d900-a47e334907b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenizer: ['Plant-based', 'does', \"n't\", 'always', 'mean', 'healthy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용하는 corpus, language, special character에 따라 규칙이 달라짐.\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "5K7EpDyfW7_U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"The punctuation in electronic health records does not always follow standard forms. Sometimes consecutive sentences in a report have a missing space after the period of the first sentence, which can cause the sentence tokenizer to treat both sentences together as a single run-on sentence. ClarityNLP detects these occurrences and separates the sentences. It also avoids separating valid abbreviations such as C.Diff., G.Jones, etc.\"\n",
        "print('sentence tokenization:',sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1KRmIvFW8CU",
        "outputId": "92f42bf0-580a-410e-d780-ef48c272264c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence tokenization: ['The punctuation in electronic health records does not always follow standard forms.', 'Sometimes consecutive sentences in a report have a missing space after the period of the first sentence, which can cause the sentence tokenizer to treat both sentences together as a single run-on sentence.', 'ClarityNLP detects these occurrences and separates the sentences.', 'It also avoids separating valid abbreviations such as C.Diff., G.Jones, etc.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTuniU_zW8Eq",
        "outputId": "568f27ce-c638-48d3-8809-b8bc7e386747"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.6.2.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.14.0)\n",
            "Building wheels for collected packages: kss\n",
            "kss\n",
            "kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.6.2-py3-none-any.whl size=42448618 sha256=d5729b7f5b0be2e6d4fff0d8b2f2e4ba15a8a97ebb9093dcdae388d93dfb1b85\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/96/36/e0934715782ac3fa2970b7a4aa4cf86725fc726bee64fdcf14\n",
            "Successfully built kss\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.2.0 kss-3.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text=\"인공지능을 활용한 자연어처리를 학습중 입니다. 그러나 한글 자연어 처리는 영어 보다 더 복잡하고 어렵습니다.\"\n",
        "print('korean tokenization:',kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC02APBMW8G9",
        "outputId": "7a93ce90-14f7-4591-bc0a-c1e5f4e936b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean tokenization: ['인공지능을 활용한 자연어처리를 학습중 입니다.', '그러나 한글 자연어 처리는 영어 보다 더 복잡하고 어렵습니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging(품사 태깅)\n",
        "# fly, 못 "
      ],
      "metadata": {
        "id": "O7tGQt_6W8JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text=\"I am actively looking for Ph.D. students, and you are a Ph.D. student.\"\n",
        "tokenized_sentence= word_tokenize(text)\n",
        "\n",
        "print('word tokenization:', tokenized_sentence)\n",
        "print('POS tagging:', pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4myiGZuW8Lh",
        "outputId": "1c83e9a5-90f6-4066-8820-1613530a92e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokenization: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', ',', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "POS tagging: [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), (',', ','), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Penn Treebank POG Tags\n",
        "# 태그마다 설명     "
      ],
      "metadata": {
        "id": "R1_QEpNoW8Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma \n",
        "\n",
        "okt=Okt()\n",
        "kkma=Kkma() #꼬꼬마 서울대에서 개발 \n",
        "# 완벽하지는 않은 한국어 자연어 처리, 사전 기준이기 때문에 생략된 부분은 잘못 분석되기도 함. 조사, 어간이 중요 \n",
        "# 자연어 처리의 어려운 문제: 동음 이의어 처리, 은유적인 표현 \n",
        "print('OKT 형태소 분석:', okt.morphs('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 품사 태깅:', okt.pos('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 명사 추출:', okt.nouns('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!')) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ru1nOBqW8Tk",
        "outputId": "7c624f10-3e2b-4fed-eb6c-eea2243c39ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석: ['인공', '지능', '공부', '는', '정말', '짜증나고', ',', '지겹고', ',', '복잡해', '!']\n",
            "OKT 품사 태깅: [('인공', 'Noun'), ('지능', 'Noun'), ('공부', 'Noun'), ('는', 'Josa'), ('정말', 'Noun'), ('짜증나고', 'Adjective'), (',', 'Punctuation'), ('지겹고', 'Adjective'), (',', 'Punctuation'), ('복잡해', 'Adjective'), ('!', 'Punctuation')]\n",
            "OKT 명사 추출: ['인공', '지능', '공부', '정말']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('OKT 형태소 분석:', kkma.morphs('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 품사 태깅:', kkma.pos('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!'))\n",
        "print('OKT 명사 추출:', kkma.nouns('인공지능 공부는 정말 짜증나고, 지겹고, 복잡해!')) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JEaE-kJW8V6",
        "outputId": "f606b1ca-700d-4f5e-9ed4-3e6f856d206a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석: ['인공지능', '공부', '는', '정말', '짜증나', '고', ',', '지겹', '고', ',', '복잡', '하', '어', '!']\n",
            "OKT 품사 태깅: [('인공지능', 'NNG'), ('공부', 'NNG'), ('는', 'JX'), ('정말', 'MAG'), ('짜증나', 'VV'), ('고', 'ECE'), (',', 'SP'), ('지겹', 'VA'), ('고', 'ECE'), (',', 'SP'), ('복잡', 'NNG'), ('하', 'XSV'), ('어', 'ECS'), ('!', 'SF')]\n",
            "OKT 명사 추출: ['인공', '인공지능', '지능', '공부', '복잡']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대소문자는 아스키 코드 값이 다르므로, 컴퓨터는 다르게 인식. 별개의 값으로 인식하고 카운트 \n",
        "# 대소문자 고려 중요한 문제. 성능에 영향 \n",
        "# 동음이의어 처리하면서 다양한 의미중에 결정\n",
        "# 자연어 noise, cleansing 범위는 도메인 문서에 따라 달라짐. 형태소 분석기가 달라짐.   "
      ],
      "metadata": {
        "id": "M92LndOjW8Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming(어간 추출) 줄기의 마디를 자르듯이 \n",
        "# 뿌리 단어를 찾아서 통합해주기 ex) am, are, is > be \n",
        "# 접사(affix) 단어에 추가적인 의미 부여 \n"
      ],
      "metadata": {
        "id": "fIBi_LRFW8cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyy0GA4JRtxJ",
        "outputId": "4ecd6129-6cc6-4864-8a81-f9f15558b9eb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words=['policy','doing','organization','have','going','love','lives','dies','has','starting']\n",
        "\n",
        "print('Before lemmatization:', words)\n",
        "print('After lemmatization:', [lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKRxEMCSRt0T",
        "outputId": "90e1748a-7a9c-4898-df9c-1cfe89446cea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'dies', 'has', 'starting']\n",
            "After lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'dy', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization(표제어) 추출 \n",
        "# POS 태깅을 통해서 과거형 분리 \n",
        "# lemmatization with POS information \n",
        "lemmatizer.lemmatize('dies','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rgr__6YIRt36",
        "outputId": "87a01a33-6965-4817-82e0-4ab79b4344c4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('watched','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZQNrmUysRt7I",
        "outputId": "3dcbef46-4d7d-45a3-e39b-3b4efdeece19"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'watch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('has','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zKq4HYuRRt-h",
        "outputId": "be4b3da4-60b5-4d63-8caf-ba895ad43b70"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"This was not the map we found in Billy Bones's cheat, but an accurate\"\n",
        "tokenized_sentence=word_tokenize(sentence)\n",
        "\n",
        "print(\"Before stemming:\",tokenized_sentence)\n",
        "print(\"After stemming:\",[stemmer.stem(word) for word in tokenized_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZP9rqwTRuCy",
        "outputId": "64684288-9f9b-4b10-aa4b-0f7fa0511c91"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before stemming: ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'cheat', ',', 'but', 'an', 'accurate']\n",
            "After stemming: ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'cheat', ',', 'but', 'an', 'accur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#porter stemmer 규칙기반 접근 \n"
      ],
      "metadata": {
        "id": "LtVDemj1RuGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "lancaster_stemmer=LancasterStemmer()\n",
        "\n",
        "words=['policy','doing','organization','have','going','love','lives','fly','dies','has','starting']\n",
        "\n",
        "print('Before lemmatization:', words)\n",
        "print('After lemmatization:', [porter_stemmer.stem(w) for w in words])\n",
        "print('Lancaster stemmer:',[lancaster_stemmer.stem(w) for w in words])\n",
        "\n",
        "# stemmer마다 다른 특징: 규칙기반 vs 러닝기반\n",
        "# steming 단어의 root 찾기 \n",
        "# lemmatization 표제어 추출\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7OGbuThRuJm",
        "outputId": "ce620bc4-cf43-4dca-a8e2-e3bd567c1ff2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before lemmatization: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'has', 'starting']\n",
            "After lemmatization: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'ha', 'start']\n",
            "Lancaster stemmer: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt\n",
        "# stopword(불용어) - 큰 의미가 ㅇ벗는 토큰, 유의미한 단어 선별을 위해 제거 \n",
        "# NLTK에서는 100개 이상 단어를 불용어로 패키지 내에 미리 정의함"
      ],
      "metadata": {
        "id": "_NqHSB0pRuMm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words_list=stopwords.words('english')\n",
        "print('Number of stopword:',len(stop_words_list))\n",
        "print('Print 10 stopwords:',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4AtetHRuPH",
        "outputId": "107c4a75-21a2-4e85-bfe8-79246207b5a2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stopword: 179\n",
            "Print 10 stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소문자 converting해서 지우기, 불용어 분류 전부 소문자 기준\n",
        "example=\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised\"\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "word_tokens=word_tokenize(example)\n",
        "result=[]\n",
        "for word in word_tokens:\n",
        "  if word not in stop_words:\n",
        "    result.append(word)\n",
        "\n",
        "print('Before stopword:', word_tokens)\n",
        "print('After stopword:', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVsUCk2dRuR9",
        "outputId": "2fa81a52-d557-4f70-bad1-ba15d9263240"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before stopword: ['Deep', 'learning', '(', 'also', 'known', 'as', 'deep', 'structured', 'learning', ')', 'is', 'part', 'of', 'a', 'broader', 'family', 'of', 'machine', 'learning', 'methods', 'based', 'on', 'artificial', 'neural', 'networks', 'with', 'representation', 'learning', '.', 'Learning', 'can', 'be', 'supervised', ',', 'semi-supervised', 'or', 'unsupervised']\n",
            "After stopword: ['Deep', 'learning', '(', 'also', 'known', 'deep', 'structured', 'learning', ')', 'part', 'broader', 'family', 'machine', 'learning', 'methods', 'based', 'artificial', 'neural', 'networks', 'representation', 'learning', '.', 'Learning', 'supervised', ',', 'semi-supervised', 'unsupervised']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text=\"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is\"\n",
        "tokenizer1=RegexpTokenizer(\"[\\w]+\")\n",
        "tokenizer2=RegexpTokenizer(\"[\\s]+\",gaps=True)\n",
        "\n",
        "print(tokenizer1.tokenize(text))\n",
        "print(tokenizer2.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ1TaaQzRuUz",
        "outputId": "432c783b-32cb-40eb-e3dc-cc5b85483e80"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is']\n",
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphanage', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://regex101.com/\n",
        "# 정규표현식 보는 사이트 참고 "
      ],
      "metadata": {
        "id": "F91mcZFbRuXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text encoding : text를 다른 형태로 변환(주로 숫자)\n",
        "# integer, one-hot encoding \n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "sample_text=\"\"\"The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, hence the \"structured\" part.\"\"\"\n",
        "\n",
        "#token\n",
        "sentences=sent_tokenize(sample_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y-tXRFKRuaI",
        "outputId": "f9fde05d-5643-48b4-eb24-862505a7dfbd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The adjective \"deep\" in deep learning refers to the use of multiple layers in the network.', 'Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.', 'Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions.', 'In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, hence the \"structured\" part.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "preprocessed_sentences=[]\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "  # 단어 토큰화\n",
        "  tokenized_sentence=word_tokenize(sentence)\n",
        "  result=[]\n",
        "\n",
        "  for word in tokenized_sentence:\n",
        "    word=word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다. \n",
        "    if word not in stop_words: # 단어 토큰화된 결과에 대해서 불용어를 제거한다. \n",
        "      if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word]=0\n",
        "        vocab[word] +=1\n",
        "    preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjvinMFZRuc0",
        "outputId": "9a846327-1284-47ac-c867-f34dc00bf635"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('voca:',vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuxLvbZRuff",
        "outputId": "dc21dd29-fe4a-42e2-8fb6-577f14fc2015"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voca: {'adjective': 1, 'deep': 4, 'learning': 3, 'refers': 1, 'use': 1, 'multiple': 1, 'layers': 3, 'network': 2, 'early': 1, 'work': 1, 'showed': 1, 'linear': 1, 'perceptron': 1, 'universal': 1, 'classifier': 1, 'nonpolynomial': 1, 'activation': 1, 'function': 1, 'one': 1, 'hidden': 1, 'layer': 1, 'unbounded': 2, 'width': 1, 'modern': 1, 'variation': 1, 'concerned': 1, 'number': 1, 'bounded': 1, 'size': 1, 'permits': 1, 'practical': 1, 'application': 1, 'optimized': 1, 'implementation': 1, 'retaining': 1, 'theoretical': 1, 'universality': 1, 'mild': 1, 'conditions': 1, 'also': 1, 'permitted': 1, 'heterogeneous': 1, 'deviate': 1, 'widely': 1, 'biologically': 1, 'informed': 1, 'connectionist': 1, 'models': 1, 'sake': 1, 'efficiency': 1, 'trainability': 1, 'understandability': 1, 'hence': 1, 'structured': 1, 'part': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\"layers\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HZ9BhMvRulJ",
        "outputId": "a32bb9ad-a50a-4f49-d82f-0ec60ed0c4c9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted=sorted(vocab.items(),key=lambda x:x[1], reverse=True) # lambda x vs x+1 비교 내림차순 정렬 \n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULyuIcVORun5",
        "outputId": "6380c58f-bcd2-4ee0-b844-389174b0c0a4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('deep', 4), ('learning', 3), ('layers', 3), ('network', 2), ('unbounded', 2), ('adjective', 1), ('refers', 1), ('use', 1), ('multiple', 1), ('early', 1), ('work', 1), ('showed', 1), ('linear', 1), ('perceptron', 1), ('universal', 1), ('classifier', 1), ('nonpolynomial', 1), ('activation', 1), ('function', 1), ('one', 1), ('hidden', 1), ('layer', 1), ('width', 1), ('modern', 1), ('variation', 1), ('concerned', 1), ('number', 1), ('bounded', 1), ('size', 1), ('permits', 1), ('practical', 1), ('application', 1), ('optimized', 1), ('implementation', 1), ('retaining', 1), ('theoretical', 1), ('universality', 1), ('mild', 1), ('conditions', 1), ('also', 1), ('permitted', 1), ('heterogeneous', 1), ('deviate', 1), ('widely', 1), ('biologically', 1), ('informed', 1), ('connectionist', 1), ('models', 1), ('sake', 1), ('efficiency', 1), ('trainability', 1), ('understandability', 1), ('hence', 1), ('structured', 1), ('part', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency >1 : # 빈도수가 작은 단어는 제외, \n",
        "    i = i+1\n",
        "  word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ4K4iFSqKl9",
        "outputId": "f5e760c4-681b-4351-f9d0-2dfb003f90c5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': 1, 'learning': 2, 'layers': 3, 'network': 4, 'unbounded': 5, 'adjective': 5, 'refers': 5, 'use': 5, 'multiple': 5, 'early': 5, 'work': 5, 'showed': 5, 'linear': 5, 'perceptron': 5, 'universal': 5, 'classifier': 5, 'nonpolynomial': 5, 'activation': 5, 'function': 5, 'one': 5, 'hidden': 5, 'layer': 5, 'width': 5, 'modern': 5, 'variation': 5, 'concerned': 5, 'number': 5, 'bounded': 5, 'size': 5, 'permits': 5, 'practical': 5, 'application': 5, 'optimized': 5, 'implementation': 5, 'retaining': 5, 'theoretical': 5, 'universality': 5, 'mild': 5, 'conditions': 5, 'also': 5, 'permitted': 5, 'heterogeneous': 5, 'deviate': 5, 'widely': 5, 'biologically': 5, 'informed': 5, 'connectionist': 5, 'models': 5, 'sake': 5, 'efficiency': 5, 'trainability': 5, 'understandability': 5, 'hence': 5, 'structured': 5, 'part': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=3\n",
        "\n",
        "#인덱스가 3 초과인 단어 제거\n",
        "words_frequency=[word for word, index in word_to_index.items() if index >= vocab_size+1]\n",
        "\n",
        "# 해당 단어에 대한 인덱스 정보를 삭제\n",
        "for w in words_frequency:\n",
        "  del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZI68HlqKqV",
        "outputId": "fe03992a-72d2-4c53-bff3-75a131cfdeb7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': 1, 'learning': 2, 'layers': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV']=len(word_to_index)+1\n",
        "print(word_to_index)\n",
        "# OOV out of vocabulary 처리 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJNJHdArqKt1",
        "outputId": "9b35707c-bab7-40b9-a9bb-4ba1abec7d42"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': 1, 'learning': 2, 'layers': 3, 'OOV': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences=[]\n",
        "for sentence in preprocessed_sentences:\n",
        "  encoded_sentence=[]\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      #단어 집합에 있는 단어라면 해당 단어의 정수를 리턴,\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "  print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vS979UaqKxG",
        "outputId": "39df96bb-fe87-4ee9-9a07-6913b21b1944"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 딥러닝 모델 매트릭스 계산 정사각행렬 형태 \n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "vocab=FreqDist(np.hstack(preprocessed_sentences))\n",
        "print(vocab[\"learning\"]) #'learning'라는 단어의 빈도수 출력\n",
        "\n",
        "vocab_size=5\n",
        "vocab=vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)\n",
        "\n",
        "word_to_index={word[0]:index+1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSBuam20qK1B",
        "outputId": "e4c9f46b-b490-4f37-87f8-9d38d8c3fe54"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "[('deep', 111), ('learning', 92), ('layers', 92), ('unbounded', 67), ('network', 51)]\n",
            "{'deep': 1, 'learning': 2, 'layers': 3, 'unbounded': 4, 'network': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "preprocessed_sent=[['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['adjective', 'deep', 'deep', 'learning', 'refers', 'use', 'multiple', 'layers', 'network'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['early', 'work', 'showed', 'linear', 'perceptron', 'universal', 'classifier', 'network', 'nonpolynomial', 'activation', 'function', 'one', 'hidden', 'layer', 'unbounded', 'width'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'modern', 'variation', 'concerned', 'unbounded', 'number', 'layers', 'bounded', 'size', 'permits', 'practical', 'application', 'optimized', 'implementation', 'retaining', 'theoretical', 'universality', 'mild', 'conditions'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part'], ['deep', 'learning', 'layers', 'also', 'permitted', 'heterogeneous', 'deviate', 'widely', 'biologically', 'informed', 'connectionist', 'models', 'sake', 'efficiency', 'trainability', 'understandability', 'hence', 'structured', 'part']]\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "# 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 tokenizer의 인자 oov_token을 사용\n",
        "# 숫자 0과 oov를 고려해서 집합\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
        "\n",
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+1)\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+2,oov_token='OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "\n",
        "print('단어 OOV의 인덱스:()'.format(tokenizer.word_index['OOV']))\n",
        "\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00gyPcq6qK4H",
        "outputId": "197ac2cd-621d-47d3-8a64-678eafd2627a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [52, 1, 1, 2, 53, 54, 55, 3, 5], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [38, 39, 40, 41, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 4, 51], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 22, 23, 24, 4, 25, 3, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
            "단어 OOV의 인덱스:()\n",
            "[[1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 2, 2, 3, 1, 1, 1, 4, 6], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 5, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 1, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt=Okt()\n",
        "tokens=okt.morphs('나는 자연어 처리를 배운다')\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BF1AP1cqK7e",
        "outputId": "a1259665-98b7-4cc7-b693-3427cb35552b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={word:index for index, word in enumerate(tokens)}\n",
        "print('단어 집합:',word_to_index)\n",
        "\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index))\n",
        "  index=word_to_index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector\n",
        "\n",
        "one_hot_encoding(\"자연어\", word_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFb5T5Q2qK_O",
        "outputId": "dfa456ba-238b-4a2d-a37c-3421e7a77f00"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index))\n",
        "  index=word_to_index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector\n",
        "\n",
        "  one_hot_encoding(\"자연어\", word_to_index)"
      ],
      "metadata": {
        "id": "DfIfgZuVqLDE"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print('단어 집합:', tokenizer.word_index)\n",
        "\n",
        "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)\n",
        "\n",
        "one_hot=to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iopwf0GcqLGE",
        "outputId": "cd723fa1-759f-4364-e2cd-0b4096efaf69"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
            "[2, 5, 1, 6, 3, 7]\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#원핫 인코딩 한계\n",
        "#단어가 늘어날수록 저장하기 위한 공간이 증가\n",
        "#단어사이의 연관성을 활용하기가 어려움\n",
        "\n",
        "#padding\n",
        "#자연어 처리과정에서 각문장, 문서의 서로 길이가 다른 경우 이를 맞춰줌\n",
        "#길이가 동일해야 하나의 행렬로 묶어서 처리가능(병렬 처리)\n",
        "#여러 문장의 길이를 임의로 동일하게 맞춰주기, 병렬연산 \n"
      ],
      "metadata": {
        "id": "m7stwK5MqLJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-aehDK3anxrI",
        "outputId": "ba2275b0-0139-4335-90df-80832e76bc58"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-fi8mch9p\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-fi8mch9p\n",
            "Collecting tensorflow==2.7.2\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.2%2Bzzzcolab20220516114640-cp37-cp37m-linux_x86_64.whl (671.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 671.4 MB 1.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (3.1.0)\n",
            "Collecting argparse>=1.4.0\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.5.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (3.17.3)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.48.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (4.1.1)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.2->pykospacing==0.5) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.2->pykospacing==0.5) (3.2.0)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.5-py3-none-any.whl size=2268638 sha256=2353d3d66dc00f78f098a27575f2c1ac938e2980b8488acf022b2c4ad6f923ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iiywyw4o/wheels/9b/93/81/a2a7dc8c66ede5bf30634d20635f32b95eac7ca2ea8844058b\n",
            "Successfully built pykospacing\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow, argparse, pykospacing\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed argparse-1.4.0 gast-0.4.0 keras-2.7.0 pykospacing-0.5 tensorflow-2.7.2+zzzcolab20220516114640 tensorflow-estimator-2.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "gast",
                  "keras",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent='김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 있다.'\n",
        "\n",
        "new_sent=sent.replace(\"\",'') # 띄어쓰기가 없는 문장 임의로 만들기\n",
        "print(new_sent)\n",
        "\n",
        "#PyKoSpasing 활용 문장 자동 띄어쓰기\n",
        "from pykospacing import Spacing\n",
        "spacing=Spacing()\n",
        "kospacing_sent=spacing(new_sent)\n",
        "\n",
        "print(sent)\n",
        "print(kospacing_sent)\n",
        "#step by step cleansing 필요 \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ljJlnA5qLNi",
        "outputId": "359cc667-a980-4733-b10d-a21e4118a0f4"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 있다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f934155b440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f930a4d7a50>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f934155b440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f930a4d7a50>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 있다.\n",
            "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결 전의 날을 앞두고 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lwpbb72qLRJ",
        "outputId": "ecc42392-40c2-4d27-edb9-fbdceb76b62e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-zt7tghs5\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-zt7tghs5\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2022.6.15)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4868 sha256=cd6c7a89eb8f549c8f986061ff504aaf3e2f9f83509210c4eecc3a9701ce214d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mqau5ib3/wheels/ab/f5/7b/d4124bb329c905301baed80e2ae45aa14e824f62ebc3ec2cc4\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: py-hanspell\n",
            "Successfully installed py-hanspell-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token(word, sentence)-소문자,오타,띄어쓰기 > POS-tagging-인덱싱 과정? > cleaning/normalization-noise,N/A > stmming/levanim > encoding "
      ],
      "metadata": {
        "id": "glLbJmH5qLUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "sent=\"맞춤뻡 틀리는 사람이 요즘 많치? 쓰고싶은 대로 쓰지뭐\"\n",
        "spelled_sent=spell_checker.check(sent)\n",
        "\n",
        "hanspell_sent=spelled_sent.checked\n",
        "print(hanspell_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAbBczsLqLYK",
        "outputId": "fa98dfb2-d14e-49dc-f329-645ce2b0b2eb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "맞춤법 틀리는 사람이 요즘 많지? 쓰고 싶은 대로 쓰지 뭐\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SOYNLP: POS tagging"
      ],
      "metadata": {
        "id": "rUq8Lea8qLbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soynlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mz0g248qLes",
        "outputId": "0f9187fa-bb14-4f11-b497-9ac30319c5b5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[K     |████████████████████████████████| 416 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.normalizer import *\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ',num_repeats=2))\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ',num_repeats=2))\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ',num_repeats=2))\n",
        "print(emoticon_normalize('엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠ',num_repeats=1))\n",
        "\n",
        "print(repeat_normalize('와하하하하하하하하하하핫',num_repeats=2))\n",
        "print(repeat_normalize('와하하하하하하하핫',num_repeats=2))\n",
        "print(repeat_normalize('와하하하하핫',num_repeats=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbzdMf9nqLit",
        "outputId": "66e0aa36-783d-4ba3-c9b9-5e58da4a7351"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어ㅋㅋ영화존잼쓰ㅠㅠ\n",
            "어ㅋㅋ영화존잼쓰ㅠㅠ\n",
            "어ㅋㅋ영화존잼쓰ㅠㅠ\n",
            "어ㅋ영화존잼쓰ㅠ\n",
            "와하하핫\n",
            "와하하핫\n",
            "와하하핫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 전처리 패키지 \n",
        "# SOYNLP, Hanspell, kkma 등\n",
        "# 확률 기반 언어모델(SLM)\n",
        "# N-gram, TF-IDF\n",
        "# 인공신경망 기반 언어모델 - BERT,GPT3,4,KoBERT,HYPERCLOVA(네이버)"
      ],
      "metadata": {
        "id": "H4OmHlKAqLl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT를 기준으로 혁신, 룰기반 학습에서, 인공지능 관련하여 논평 작성도 가능한 수준. \n",
        "#언어모델이 두문장을 비교하여 적절 확률 계산, 비교 및 판단 - 기계번역, 오타교정,음성인식\n",
        "#통계적 언어모델 - 조건부 확률 계산 \n",
        "#n-gram: 확률을 계산하기 위한 단어의 포함범위를 조절하여 최대 확률을 가지도록 유도 \n",
        "#n: 문자의 갯수, 몇개까지 고려할지, 더 짧은 단어 시퀀스가 존재할 확률이 더 높음 \n",
        "#TF-IDF(term frequency): 단어의 빈도와 역문서 빈도를 사용하여 문서집합 내 단어들마다 중요한 정도를 가중치로 부여 \n",
        "#너무 흔한단어는 stop word 처리 \n",
        "#한문서에서 단어가 자주 등장하면 중요, 특정단어가 여러문서에서 등장하면 중요하지 않음.\n",
        "#idf와 df는 반비례 역수관계 \n"
      ],
      "metadata": {
        "id": "Y8lrIbSvqLo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # 데이터프레임 사용을 위해\n",
        "from math import log #IDF 계산을 위해\n",
        "\n",
        "docs=[\n",
        "    '먹고 싶은 사과',\n",
        "    '먹고 싶은 바나나',\n",
        "    '길고 노란 바나나 바나나',\n",
        "    '저는 과일이 좋아요'\n",
        "]\n",
        "\n",
        "vocab=list(set(w for doc in docs for w in doc.split()))\n",
        "vocab.sort()"
      ],
      "metadata": {
        "id": "qPzI6l6RqLsN"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 총 문서의 수\n",
        "N=len(docs)\n",
        "\n",
        "def tf(t,d):\n",
        "  return d.count(t)\n",
        "\n",
        "def idf(t):\n",
        "  df=0\n",
        "  for doc in docs:\n",
        "    df += t in doc\n",
        "  return log(N/(df+1))\n",
        "\n",
        "def tfidf(t,d):\n",
        "  return tf(t,d)*idf(t)"
      ],
      "metadata": {
        "id": "topZxk76qLvw"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "\n",
        "# 각 문서에 대해서 아래 연산을 반복\n",
        "for i in range(N):\n",
        "  result.append([])\n",
        "  d=docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t=vocab[j]\n",
        "    result[-1].append(tf(t,d))\n",
        "\n",
        "tf_=pd.DataFrame(result,columns=vocab)"
      ],
      "metadata": {
        "id": "8CjGtA51qLzP"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "for j in range(len(vocab)):\n",
        "  t=vocab[j]\n",
        "  result.append(idf(t))\n",
        "\n",
        "idf_=pd.DataFrame(result,index=vocab,columns=['IDF'])\n",
        "idf_\n",
        "# idf값이 클수록 중요, df값이 클수록 안중요 \n",
        "# idf값은 df값의 역수 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "KCBQQFvQRuq9",
        "outputId": "a76aaa04-06f3-4d33-d551-4c7f4076eecb"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          IDF\n",
              "과일이  0.693147\n",
              "길고   0.693147\n",
              "노란   0.693147\n",
              "먹고   0.287682\n",
              "바나나  0.287682\n",
              "사과   0.693147\n",
              "싶은   0.287682\n",
              "저는   0.693147\n",
              "좋아요  0.693147"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9482ce7-bd05-4b59-927a-f8e9e669672f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>과일이</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>길고</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>노란</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>먹고</th>\n",
              "      <td>0.287682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>바나나</th>\n",
              "      <td>0.287682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>사과</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>싶은</th>\n",
              "      <td>0.287682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>저는</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>좋아요</th>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9482ce7-bd05-4b59-927a-f8e9e669672f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9482ce7-bd05-4b59-927a-f8e9e669672f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9482ce7-bd05-4b59-927a-f8e9e669672f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "for i in range(N):\n",
        "  result.append([])\n",
        "  d=docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t=vocab[j]\n",
        "    result[-1].append(tfidf(t,d))\n",
        "\n",
        "tfidf_=pd.DataFrame(result,columns=vocab)\n",
        "tfidf_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "yahQKNfqwdju",
        "outputId": "441196ba-a032-48d5-be07-72c8e3476d0a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        과일이        길고        노란  ...        싶은        저는       좋아요\n",
              "0  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.287682  0.000000  0.000000\n",
              "2  0.000000  0.693147  0.693147  ...  0.000000  0.000000  0.000000\n",
              "3  0.693147  0.000000  0.000000  ...  0.000000  0.693147  0.693147\n",
              "\n",
              "[4 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c88184d-fb10-4131-b81c-1adeb5613886\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>과일이</th>\n",
              "      <th>길고</th>\n",
              "      <th>노란</th>\n",
              "      <th>먹고</th>\n",
              "      <th>바나나</th>\n",
              "      <th>사과</th>\n",
              "      <th>싶은</th>\n",
              "      <th>저는</th>\n",
              "      <th>좋아요</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.575364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.693147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c88184d-fb10-4131-b81c-1adeb5613886')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8c88184d-fb10-4131-b81c-1adeb5613886 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8c88184d-fb10-4131-b81c-1adeb5613886');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-7hJJIpwdmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iK5sR0sswdpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2-r5usqwdr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFUUuWLtwduC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STZE-DUnwdwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdXEOaz-wdy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aewoYLPDwd1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7vpCdzCwd4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExlcQ4juwd6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpjrzHoKwd9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvVncb9Rwd_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUSj0hz-weCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9A-BRNJweFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1OaRLvcweHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5B9gwB2JweJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yYj_Oz1weMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YuM-Uf8iwePb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qiRUQbGqweR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzT5B1LXRutp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}